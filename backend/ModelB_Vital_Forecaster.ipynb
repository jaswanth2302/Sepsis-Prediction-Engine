{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model B: Vital Sign Forecaster\n",
                "\n",
                "**Purpose**: Predict NEXT vital values (autoregressive for 5-second simulation)\n",
                "\n",
                "**Output**: `vital_forecaster.pkl`\n",
                "\n",
                "**Dataset**: Your cleaned `merged_sepsis_data.csv`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pandas numpy scikit-learn xgboost matplotlib --quiet\n",
                "print(\"✅ Packages installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "import os\n",
                "from datetime import datetime\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import mean_absolute_error, r2_score\n",
                "from sklearn.multioutput import MultiOutputRegressor\n",
                "from xgboost import XGBRegressor\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(f\"✅ Libraries imported!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_PATH = '../merged_sepsis_data.csv'\n",
                "\n",
                "print(f\"Loading: {DATA_PATH}\")\n",
                "df = pd.read_csv(DATA_PATH)\n",
                "\n",
                "print(f\"\\n✅ Loaded: {df.shape[0]:,} rows × {df.shape[1]} cols\")\n",
                "print(f\"   Patients: {df['Patient_ID'].nunique():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Create Target Variables (Next Values)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sort by patient and time\n",
                "df = df.sort_values(['Patient_ID', 'ICULOS']).reset_index(drop=True)\n",
                "\n",
                "# Create \"next\" columns by shifting within each patient\n",
                "df['HR_next'] = df.groupby('Patient_ID')['HR'].shift(-1)\n",
                "df['Resp_next'] = df.groupby('Patient_ID')['Resp'].shift(-1)\n",
                "df['Temp_next'] = df.groupby('Patient_ID')['Temp'].shift(-1)\n",
                "df['SBP_next'] = df.groupby('Patient_ID')['SBP'].shift(-1)\n",
                "df['O2Sat_next'] = df.groupby('Patient_ID')['O2Sat'].shift(-1)\n",
                "\n",
                "# Drop rows with NaN targets (last row per patient)\n",
                "df = df.dropna(subset=['HR_next', 'Resp_next', 'Temp_next', 'SBP_next', 'O2Sat_next'])\n",
                "\n",
                "print(f\"✅ Created target columns (shifted)\")\n",
                "print(f\"   Remaining rows: {len(df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add ShockIndex if not already present\n",
                "if 'ShockIndex' not in df.columns:\n",
                "    df['ShockIndex'] = df['HR'] / df['SBP'].replace(0, np.nan)\n",
                "    df['ShockIndex'] = df['ShockIndex'].fillna(0.67).clip(0, 5)\n",
                "    print(\"✅ Added ShockIndex\")\n",
                "\n",
                "print(f\"Available columns: {list(df.columns)[:15]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Prepare Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Features (current values)\n",
                "FEATURE_COLUMNS = [\n",
                "    'ICULOS',\n",
                "    'HR',\n",
                "    'Resp',\n",
                "    'Temp',\n",
                "    'SBP',\n",
                "    'DBP',\n",
                "    'O2Sat',\n",
                "    'MAP',\n",
                "    'ShockIndex'\n",
                "]\n",
                "\n",
                "# Targets (next values)\n",
                "TARGET_COLUMNS = [\n",
                "    'HR_next',\n",
                "    'Resp_next',\n",
                "    'Temp_next',\n",
                "    'SBP_next',\n",
                "    'O2Sat_next'\n",
                "]\n",
                "\n",
                "# Extract\n",
                "X = df[FEATURE_COLUMNS].copy()\n",
                "y = df[TARGET_COLUMNS].copy()\n",
                "\n",
                "# Fill NaN\n",
                "X = X.fillna(X.median())\n",
                "y = y.fillna(y.median())\n",
                "\n",
                "print(f\"Features: {X.shape}\")\n",
                "print(f\"Targets: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample for faster training\n",
                "SAMPLE_SIZE = 150000\n",
                "\n",
                "if len(X) > SAMPLE_SIZE:\n",
                "    idx = np.random.choice(len(X), SAMPLE_SIZE, replace=False)\n",
                "    X = X.iloc[idx]\n",
                "    y = y.iloc[idx]\n",
                "    print(f\"✅ Sampled to {len(X):,} rows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/Test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Train: {X_train_scaled.shape}\")\n",
                "print(f\"Test: {X_test_scaled.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Train Multi-Output Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training Multi-Output XGBoost...\")\n",
                "\n",
                "base_model = XGBRegressor(\n",
                "    n_estimators=100,\n",
                "    max_depth=5,\n",
                "    learning_rate=0.1,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbosity=0\n",
                ")\n",
                "\n",
                "model = MultiOutputRegressor(base_model)\n",
                "model.fit(X_train_scaled, y_train)\n",
                "\n",
                "print(\"✅ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model.predict(X_test_scaled)\n",
                "\n",
                "print(\"Performance Metrics:\")\n",
                "print(\"-\"*40)\n",
                "\n",
                "metrics = {}\n",
                "for i, col in enumerate(TARGET_COLUMNS):\n",
                "    mae = mean_absolute_error(y_test.iloc[:, i], y_pred[:, i])\n",
                "    r2 = r2_score(y_test.iloc[:, i], y_pred[:, i])\n",
                "    metrics[col] = {'MAE': mae, 'R2': r2}\n",
                "    print(f\"{col}: MAE={mae:.2f}, R²={r2:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Test Autoregressive Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def autoregressive_forecast(initial_vitals: dict, n_steps: int = 5) -> list:\n",
                "    \"\"\"\n",
                "    Generate n_steps of future predictions using autoregressive loop.\n",
                "    This is what the backend will use for 5-second simulation!\n",
                "    \"\"\"\n",
                "    predictions = []\n",
                "    current = initial_vitals.copy()\n",
                "    \n",
                "    for step in range(n_steps):\n",
                "        # Build feature vector\n",
                "        sbp = current.get('SBP', 120)\n",
                "        dbp = current.get('DBP', 80)\n",
                "        hr = current.get('HR', 80)\n",
                "        \n",
                "        MAP = (sbp + 2 * dbp) / 3\n",
                "        ShockIndex = hr / sbp if sbp > 0 else 0.67\n",
                "        \n",
                "        features = [\n",
                "            current.get('ICULOS', 1),\n",
                "            hr,\n",
                "            current.get('Resp', 18),\n",
                "            current.get('Temp', 37.0),\n",
                "            sbp,\n",
                "            dbp,\n",
                "            current.get('O2Sat', 97),\n",
                "            MAP,\n",
                "            ShockIndex\n",
                "        ]\n",
                "        \n",
                "        # Scale and predict\n",
                "        X_scaled = scaler.transform([features])\n",
                "        pred = model.predict(X_scaled)[0]\n",
                "        \n",
                "        next_vitals = {\n",
                "            'sequence_index': step + 1,\n",
                "            'HR': round(float(pred[0]), 1),\n",
                "            'Resp': round(float(pred[1]), 1),\n",
                "            'Temp': round(float(pred[2]), 2),\n",
                "            'SBP': round(float(pred[3]), 0),\n",
                "            'O2Sat': round(float(pred[4]), 1)\n",
                "        }\n",
                "        \n",
                "        predictions.append(next_vitals)\n",
                "        \n",
                "        # AUTOREGRESSIVE: Feed output back as input\n",
                "        current = {\n",
                "            'HR': next_vitals['HR'],\n",
                "            'Resp': next_vitals['Resp'],\n",
                "            'Temp': next_vitals['Temp'],\n",
                "            'SBP': next_vitals['SBP'],\n",
                "            'DBP': dbp,\n",
                "            'O2Sat': next_vitals['O2Sat'],\n",
                "            'ICULOS': current.get('ICULOS', 1) + 0.1\n",
                "        }\n",
                "    \n",
                "    return predictions\n",
                "\n",
                "# Test\n",
                "test_input = {\n",
                "    'HR': 105, 'Resp': 22, 'Temp': 38.3, \n",
                "    'SBP': 100, 'DBP': 65, 'O2Sat': 93, 'ICULOS': 10\n",
                "}\n",
                "\n",
                "print(\"Initial:\", test_input)\n",
                "print(\"\\n5-Step Forecast:\")\n",
                "for p in autoregressive_forecast(test_input, 5):\n",
                "    print(f\"  Step {p['sequence_index']}: HR={p['HR']}, Temp={p['Temp']}, SBP={p['SBP']}, O2={p['O2Sat']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_FILENAME = 'vital_forecaster.pkl'\n",
                "\n",
                "model_package = {\n",
                "    'model': model,\n",
                "    'scaler': scaler,\n",
                "    'feature_columns': FEATURE_COLUMNS,\n",
                "    'target_columns': TARGET_COLUMNS,\n",
                "    'version': '1.0',\n",
                "    'trained_at': datetime.now().isoformat(),\n",
                "    'metrics': metrics\n",
                "}\n",
                "\n",
                "with open(MODEL_FILENAME, 'wb') as f:\n",
                "    pickle.dump(model_package, f)\n",
                "\n",
                "file_size = os.path.getsize(MODEL_FILENAME) / 1024\n",
                "\n",
                "print(\"=\"*40)\n",
                "print(\"VITAL FORECASTER SAVED!\")\n",
                "print(\"=\"*40)\n",
                "print(f\"File: {MODEL_FILENAME}\")\n",
                "print(f\"Size: {file_size:.1f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ✅ Model B Complete!\n",
                "\n",
                "Both models are now ready:\n",
                "- `sepsis_model.pkl` - Risk classifier\n",
                "- `vital_forecaster.pkl` - Time-series forecaster\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}